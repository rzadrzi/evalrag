# evalrag/core/eval.py


"""
Evaluation of RAG system has tow main parts:
1. Component wise:
    1. Retrieval:
        1. Context Recall
        2. Context Percision
    
    2. Generation:
        1. Faithfulness
        2. Answer Relevancy

        
2. End to End:
    1. Answer Semantic Similarity
    2. Answer Correctness

    
"""

"""
Steps to evaluate RAG:
    1. An evaluation dataset with question - answer couples (QA couples):
        The evaluation dataset will be synthetically generated by an LLM ðŸ¤–, and questions will be filtered out by other LLMs ðŸ¤–
    2. An evaluator to compute the accuracy of our system on the above evaluation dataset:
        An LLM-as-a-judge agent ðŸ¤– will then perform the evaluation on this synthetic dataset.

"""


from typing import List, Dict, Any  # noqa: E402


class Evaluator:

    """
    Evaluation helper for assessing RAG answers.

    Responsibilities:
      - Use a judge model to evaluate answers.
      - Compute metrics like correctness, faithfulness, and relevance.
      - Aggregate scores based on configured weights.
    Typical usage:
      1. `evaluate_answer(question, answer, contexts)` to get evaluation scores.
    Args:
        config: Configuration object or mapping used by evaluation routines.
    """

    def __init__(self, config) -> None:

        """
        Initialize the Evaluator.

        Args:
            config: Configuration object or mapping used by evaluation processes.

        The provided `config` is stored on the instance for later use by
        other methods in this class.
        """
        self.config = config
    
    def cost_correctness(self):
        pass
    
    def cost_faithfulness(self):
        pass
    
    def cost_relevance(self):
        pass 

    def evaluate_answer(self, question: str, answer: str, contexts: List[Dict]) -> Dict[str, Any]:
        """
        Evaluate the given `answer` to `question` using `contexts`.
        Args:
            question: The user question text.
            answer: The generated answer text.
            contexts: List of context chunks used to generate the answer.
        Returns:
            A dict containing evaluation scores and feedback.
        """
        # placeholder implementation
        scores = {
            "correctness": 4,
            "faithfulness": 5,
            "relevance": 4,
        }
        feedback = "The answer is mostly correct and faithful to the context."
        result = {
            "scores": scores,
            "feedback": feedback,
        }
        return result